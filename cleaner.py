import os
import asyncio
import aiohttp
import re
import logging
import random

# --- CONFIGURATION ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("Cleaner")

INPUT_FILE = "verified_ru.txt"
BACKUP_FILE = "verified_ru_backup.txt"

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15",
]

# --- PRE-FILTERS (–ß—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –º—É—Å–æ—Ä) ---
# –†–∞—Å—à–∏—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ä–∞–∑—É
SKIP_EXTENSIONS = {'.sh', '.md', '.py', '.jpg', '.png', '.gif', '.svg', '.zip', '.tar.gz'}
# –°–ª–æ–≤–∞ –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω—ã
SKIP_KEYWORDS = {'readme', 'install', 'tutorial', 'instruction', 'changelog', 'license'}

def get_random_header():
    return {"User-Agent": random.choice(USER_AGENTS)}

def should_skip_url(url):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç URL –ø–µ—Ä–µ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º."""
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    parsed = url.lower()
    for ext in SKIP_EXTENSIONS:
        if parsed.endswith(ext):
            return True, f"Skipped extension: {ext}"
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å URL (–∏–º—è —Ñ–∞–π–ª–∞)
    filename = url.split('/')[-1]
    for kw in SKIP_KEYWORDS:
        if kw in filename.lower():
            return True, f"Skipped keyword: {kw}"
            
    return False, ""

def get_md5_head(content):
    import hashlib
    head = content[:500].encode('utf-8', errors='ignore')
    return hashlib.md5(head).hexdigest()

def is_valid_content(content):
    """–°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ."""
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ HTML (404 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
    if "<!DOCTYPE html" in content or "<html>" in content.lower():
        return False, "HTML Page (likely 404)"
        
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    BAD_DOMAINS = ['.ir', 'zula.ir']
    if any(d in content for d in BAD_DOMAINS): 
        return False, "Bad Domain found"
        
    # 3. –ü–æ–∏—Å–∫ VLESS —Å—Å—ã–ª–æ–∫
    vless_links = re.findall(r'vless://[^\s<>"]+', content)
    if not vless_links:
        return False, "No VLESS links found"
    
    valid_count = 0
    BLACK_SNI = ['google.com', 'youtube.com', 'pornhub', 'bet', 'casino']
    
    for link in vless_links:
        if "security=reality" not in link and "type=grpc" not in link: continue
        if any(b in link for b in BLACK_SNI): continue
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–≥–ª—É—à–∫–∏
        if any(ph in link for ph in ['uuid', 'server', 'example.com', '1.1.1.1']): continue
        
        valid_count += 1

    if valid_count == 0:
        return False, "No valid Reality configs"
    
    return True, f"Found {valid_count} nodes"

# --- CLEANER CORE ---

async def check_url(session, url):
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    skip, reason = should_skip_url(url)
    if skip:
        return False, reason

    try:
        async with session.get(url, headers=get_random_header(), timeout=8) as resp:
            if resp.status != 200:
                return False, f"HTTP {resp.status}"
            
            content = await resp.text(errors='ignore')
            
            if len(content) < 50:
                return False, "Too small content"
                
            return is_valid_content(content)
            
    except asyncio.TimeoutError:
        return False, "Timeout"
    except Exception as e:
        return False, str(e)

async def main():
    if not os.path.exists(INPUT_FILE):
        logger.error(f"File {INPUT_FILE} not found!")
        return

    # 1. –ß—Ç–µ–Ω–∏–µ
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]
    
    logger.info(f"üõÅ Starting genocide for {len(urls)} URLs...")
    
    # –ë—ç–∫–∞–ø
    with open(BACKUP_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(urls))
    logger.info(f"üì¶ Backup saved to {BACKUP_FILE}")

    survivors = []
    seen_hashes = set()
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i, url in enumerate(urls):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL –¥–æ –∑–∞–ø—Ä–æ—Å–∞ (—ç–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏)
            if should_skip_url(url)[0]:
                logger.info(f"  ‚ö° [SKIP] {url.split('/')[-1]}...")
                continue

            task = check_url(session, url)
            tasks.append((i, url, task))
            
            # –ü–∞—á–∫–∏
            if len(tasks) >= 20 or i == len(urls) - 1:
                results = await asyncio.gather(*[t[2] for t in tasks])
                
                for idx, (orig_i, url, _) in enumerate(tasks):
                    is_alive, reason = results[idx]
                    
                    if is_alive:
                        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ö–µ—à—É (—á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–∂–∞—Ç—å 5 –≤–µ—Ä—Å–∏–π –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞)
                        # –ù–æ –¥–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç, –∞ –º—ã –µ–≥–æ —É–∂–µ —Å–∫–∞—á–∞–ª–∏ –≤–Ω—É—Ç—Ä–∏ is_valid_content...
                        # –ß—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å 2 —Ä–∞–∑–∞, is_valid_content –¥–æ–ª–∂–Ω–∞ –≤–µ—Ä–Ω—É—Ç—å —Ö–µ—à –∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç.
                        # –£–ø—Ä–æ—Å—Ç–∏–º: –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤–æ—Å—Ç—å. 
                        # –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ, –Ω–∞–¥–æ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –ª–æ–≥–∏–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
                        
                        # –°—á–∏—Ç–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ alive - –æ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ URL
                        survivors.append(url)
                    else:
                        logger.info(f"  ‚ùå [{orig_i+1}] KILLED: {url[:50]}... ({reason})")
                
                tasks = []
                await asyncio.sleep(1)

    # 3. –ó–∞–ø–∏—Å—å
    with open(INPUT_FILE, "w", encoding="utf-8") as f:
        for url in survivors:
            f.write(url + "\n")

    killed = len(urls) - len(survivors)
    logger.info("="*40)
    logger.info(f"ü™¶ GENOCIDE COMPLETED:")
    logger.info(f"  Before: {len(urls)}")
    logger.info(f"  Killed:  {killed}")
    logger.info(f"  Alive:   {len(survivors)}")
    logger.info("="*40)

if __name__ == "__main__":
    asyncio.run(main()) 
